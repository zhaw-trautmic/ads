{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datentypen, Dimension, Seperators checken bei Bereinigung (manuel), ob Werte auch die, die wir erwarten.\n",
    "EDA alles was nicht Modellierung und Statistisch ist. nur Histogramm, Boxplot etc. Mindestens 2 (z.b: Limeplot. werte Range mit Boxplot etc. Pairplot, Zeitlicher Lag von Closing/Opening Prises, Volumen, Volumen-Preis skettered Plot correlation). Also nur Explorative Datenanalyse für Verständnis und Hypothesen erstellen kann.\n",
    "Modell: Erweitertest Framework nehmen für Zusatzpunkte. Gäbe noch  nn => Nochmals prüfen. Was steckt dahinter? Pytorch wäre eine Option oder Lstm (Long short term memory network) \n",
    "Müssen wissen, welches NN verwendet wird => kommentieren (Torch.nn)\n",
    "7: Regression r^2 etc. \n",
    "classification wäre confusion matrix. Wir haben Regressionproblem, RMSE, MAPE (sicher dazu nehmen)\n",
    "Kreativitätspunkt: Technische Analyse einbauen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Datenerhebung mittels API & Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "from pandas_datareader import data\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Yahoo Finance API: Aktienkurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\urllib\\request.py:1346\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1346\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[0;32m   1347\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m   1348\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\http\\client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mConnect to a host on a given (SSL) port.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1447\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\http\\client.py:946\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[39m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(\n\u001b[0;32m    947\u001b[0m     (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address)\n\u001b[0;32m    948\u001b[0m \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\socket.py:823\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    822\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m getaddrinfo(host, port, \u001b[39m0\u001b[39;49m, SOCK_STREAM):\n\u001b[0;32m    824\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 954\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[0;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m interval \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m query_string \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://query1.finance.yahoo.com/v7/finance/download/\u001b[39m\u001b[39m{\u001b[39;00mticker\u001b[39m}\u001b[39;00m\u001b[39m?period1=\u001b[39m\u001b[39m{\u001b[39;00mperiod1\u001b[39m}\u001b[39;00m\u001b[39m&period2=\u001b[39m\u001b[39m{\u001b[39;00mperiod2\u001b[39m}\u001b[39;00m\u001b[39m&interval=\u001b[39m\u001b[39m{\u001b[39;00minterval\u001b[39m}\u001b[39;00m\u001b[39m&events=history&includeAdjustedClose=true\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(query_string)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Save data to a CSV file\u001b[39;00m\n\u001b[0;32m     15\u001b[0m data\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mAAPL Prices.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\io\\common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     codecs\u001b[39m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    712\u001b[0m \u001b[39m# open URLs\u001b[39;00m\n\u001b[1;32m--> 713\u001b[0m ioargs \u001b[39m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    714\u001b[0m     path_or_buf,\n\u001b[0;32m    715\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    716\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    717\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    718\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    719\u001b[0m )\n\u001b[0;32m    721\u001b[0m handle \u001b[39m=\u001b[39m ioargs\u001b[39m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    722\u001b[0m handles: \u001b[39mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\io\\common.py:363\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    362\u001b[0m req_info \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[39m=\u001b[39mstorage_options)\n\u001b[1;32m--> 363\u001b[0m \u001b[39mwith\u001b[39;00m urlopen(req_info) \u001b[39mas\u001b[39;00m req:\n\u001b[0;32m    364\u001b[0m     content_encoding \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mContent-Encoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    365\u001b[0m     \u001b[39mif\u001b[39;00m content_encoding \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    366\u001b[0m         \u001b[39m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\pandas\\io\\common.py:265\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39mthe stdlib.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\urllib\\request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    516\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 517\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    519\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    520\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\urllib\\request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    533\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 534\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    535\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\urllib\\request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1390\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\urllib\\request.py:1349\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[0;32m   1347\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1350\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m   1351\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "\n",
    "# Save data to a CSV file\n",
    "data.to_csv('AAPL Prices.csv')\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "https://www.politifact.com/factchecks/list/?page=1\n",
      "30\n",
      "processing page : 2\n",
      "https://www.politifact.com/factchecks/list/?page=2\n",
      "30\n",
      "processing page : 3\n",
      "https://www.politifact.com/factchecks/list/?page=3\n",
      "30\n",
      "processing page : 4\n",
      "https://www.politifact.com/factchecks/list/?page=4\n",
      "30\n",
      "processing page : 5\n",
      "https://www.politifact.com/factchecks/list/?page=5\n",
      "30\n",
      "processing page : 6\n",
      "https://www.politifact.com/factchecks/list/?page=6\n",
      "30\n",
      "processing page : 7\n",
      "https://www.politifact.com/factchecks/list/?page=7\n",
      "30\n",
      "processing page : 8\n",
      "https://www.politifact.com/factchecks/list/?page=8\n",
      "30\n",
      "processing page : 9\n",
      "https://www.politifact.com/factchecks/list/?page=9\n",
      "30\n",
      "processing page : 10\n",
      "https://www.politifact.com/factchecks/list/?page=10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "\n",
    "pagesToGet = 10 # Anzahl der zu untersuchenden Seiten\n",
    "search_term = \"Apple\" # Stichwort, nach dem gesucht wird\n",
    "upperframe = []\n",
    "\n",
    "# Öffnen einer Datei, um die Ergebnisse zu speichern\n",
    "filename = \"NEWS.csv\"\n",
    "f = open(filename, \"w\", encoding='utf-8')\n",
    "headers = \"Statement,Link,Date,Source,Label\\n\"\n",
    "f.write(headers)\n",
    "\n",
    "# Schleife über die Seiten\n",
    "for page in range(1, pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.politifact.com/factchecks/list/?page=' + str(page)\n",
    "    print(url)\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "    except Exception as e:\n",
    "        error_type, error_obj, error_info = sys.exc_info()\n",
    "        print ('ERROR FOR LINK:', url)\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "        continue\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    links = soup.find_all('li', attrs={'class': 'o-listicle__item'})\n",
    "    print(len(links))\n",
    "    \n",
    "    # Schleife über die Links auf der Seite\n",
    "    for j in links:\n",
    "        Statement = j.find(\"div\", attrs={'class': 'm-statement__quote'}).text.strip()\n",
    "        if search_term.lower() not in Statement.lower():\n",
    "            continue\n",
    "        Link = \"https://www.politifact.com\"\n",
    "        Link += j.find(\"div\", attrs={'class': 'm-statement__quote'}).find('a')['href'].strip()\n",
    "        Date = j.find('div', attrs={'class': 'm-statement__body'}).find('footer').text[-14:-1].strip()\n",
    "        Source = j.find('div', attrs={'class': 'm-statement__meta'}).find('a').text.strip()\n",
    "        Label = j.find('div', attrs={'class': 'm-statement__content'}).find('img', attrs={'class': 'c-image__original'}).get('alt').strip()\n",
    "        upperframe.append((Statement, Link, Date, Source, Label))\n",
    "        f.write(Statement.replace(\",\", \"^\") + \",\" + Link + \",\" + Date.replace(\",\", \"^\") + \",\" + Source.replace(\",\", \"^\") + \",\" + Label.replace(\",\", \"^\") + \"\\n\")\n",
    "f.close()\n",
    "\n",
    "# Erstellen eines DataFrames aus den Ergebnissen\n",
    "data = pd.DataFrame(upperframe, columns=['Statement', 'Link', 'Date', 'Source', 'Label'])\n",
    "\n",
    "# Öffnen einer neuen Datei, um die Sentiments der Überschriften zu speichern\n",
    "with open('APPL News.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Sentiment', 'Sentiment Label'])  # Schreibt die Spaltenüberschriften\n",
    "    for title in data['Statement']:\n",
    "        blob = TextBlob(title)\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        if sentiment < -0.2:\n",
    "            sentiment_label = 'negativ'\n",
    "        elif sentiment > 0.2:\n",
    "            sentiment_label = 'positiv'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        writer.writerow([title, sentiment, sentiment_label])\n",
    "\n",
    "# Für Zusatzaufgabe 6 NLP\n",
    "\n",
    "# with open('APPL News.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['Title', 'Sentiment', 'Sentiment Label'])  # Schreibt die Spaltenüberschriften\n",
    "#     for title in titles:\n",
    "#         blob = TextBlob(title)\n",
    "#         sentiment = blob.sentiment.polarity\n",
    "#         if sentiment < -0.2:\n",
    "#             sentiment_label = 'negativ'\n",
    "#         elif sentiment > 0.2:\n",
    "#             sentiment_label = 'positiv'\n",
    "#         else:\n",
    "#             sentiment_label = 'neutral'\n",
    "#         writer.writerow([title, sentiment, sentiment_label])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Datenaufbereitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entfernen NAs und Duplikate, Erstellen neuer Variablen, Anreicherung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "Statement            object\n",
      "Link                 object\n",
      "Date         datetime64[ns]\n",
      "Source               object\n",
      "Label                object\n",
      "dtype: object\n",
      "\n",
      "Data Dimensions:\n",
      "(0, 5)\n",
      "---------------------------BEFORE CLEAN---------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Statement  0 non-null      object        \n",
      " 1   Link       0 non-null      object        \n",
      " 2   Date       0 non-null      datetime64[ns]\n",
      " 3   Source     0 non-null      object        \n",
      " 4   Label      0 non-null      object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 0.0+ bytes\n",
      "---------------------------AFTER CLEAN---------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Statement  0 non-null      object        \n",
      " 1   Link       0 non-null      object        \n",
      " 2   Date       0 non-null      datetime64[ns]\n",
      " 3   Source     0 non-null      object        \n",
      " 4   Label      0 non-null      object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Cleaning der Quotes\n",
    "df = data\n",
    "\n",
    "# Check for format and change it (Frage: notwendig?)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "raw = df\n",
    "\n",
    "# Datentypen und Dimensionen anzeigen lassen\n",
    "print(\"Data Types:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\nData Dimensions:\")\n",
    "print(data.shape)\n",
    "\n",
    "# Descriptive Statistics (Frage: notwendig?)\n",
    "# df.describe()\n",
    "\n",
    "# Cleaning Process\n",
    "print('---------------------------BEFORE CLEAN---------------------------')\n",
    "raw.info()\n",
    "print('---------------------------AFTER CLEAN---------------------------')\n",
    "\n",
    "# Data cleaning and missing values using forward fill\n",
    "df = df.drop_duplicates()\n",
    "df = df\n",
    "df = df.dropna()\n",
    "df = df\n",
    "df = df.ffill()\n",
    "df = df\n",
    "clean = df\n",
    "clean.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 DB - PostgreSQL DB initiate -> In Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"db\" to address: Unknown host\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# Connect DB\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m conn \u001b[39m=\u001b[39m psycopg2\u001b[39m.\u001b[39;49mconnect(\u001b[39m\"\u001b[39;49m\u001b[39mhost=db dbname=postgres user=admin password=secret\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m \u001b[39m# Insert data to appl_prices\u001b[39;00m\n\u001b[0;32m     20\u001b[0m engine \u001b[39m=\u001b[39m create_engine(\u001b[39m'\u001b[39m\u001b[39mpostgresql://admin:secret@db:5432/postgres\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39mconnection_factory, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: could not translate host name \"db\" to address: Unknown host\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import fnmatch\n",
    "import tempfile\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = \"/home/jovyan\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Connect DB\n",
    "conn = psycopg2.connect(\"host=db dbname=postgres user=admin password=secret\")\n",
    "\n",
    "# Insert data to appl_prices\n",
    "engine = create_engine('postgresql://admin:secret@db:5432/postgres')\n",
    "data.to_sql('appl_prices', engine, if_exists='replace')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Spalte positive hinzufügen\n",
    "cur.execute(\"ALTER TABLE appl_prices ADD COLUMN Positive INTEGER DEFAULT 0;\")\n",
    "\n",
    "# Änderungen speichern\n",
    "conn.commit()\n",
    "\n",
    "# Datenbankverbindung schliessen\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect DB\n",
    "conn = psycopg2.connect(\"host=db dbname=postgres user=admin password=secret\")\n",
    "\n",
    "# Update DB\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"UPDATE appl_prices SET positive = CASE WHEN \"Close\" >= \"Open\" THEN 1 ELSE 0 END;\"\"\")\n",
    "\n",
    "# Änderungen speichern\n",
    "conn.commit()\n",
    "\n",
    "# Datenbankverbindung schliessen\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect DB\n",
    "conn = psycopg2.connect(\"host=db dbname=postgres user=admin password=secret\")\n",
    "\n",
    "# Selct DB content\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT * FROM appl_prices LIMIT 10;\"\"\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Datenbankverbindung schliessen\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bsp. Histogramm und weiter Grafiken, Zeitreihen, lags vom close preis, Volumen, volumen mit preis vergleichen \n",
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "data.to_csv('APPL Prices.csv')\n",
    "print(data)\n",
    "\n",
    "# Histogram\n",
    "plt.hist(data['Close'], bins=50)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('AAPL Prices Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(data['Close'], vert=False)\n",
    "plt.xlabel('Price')\n",
    "plt.title('AAPL Prices Boxplot')\n",
    "plt.show()\n",
    "\n",
    "# Time Series\n",
    "plt.plot(data['Date'], data['Close'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('AAPL Prices Time Series')\n",
    "plt.show()\n",
    "\n",
    "# Lineplot\n",
    "sns.lineplot(data=data, x='Date', y='Close')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('AAPL Prices Lineplot')\n",
    "plt.show()\n",
    "\n",
    "# Time Lag Plot\n",
    "from pandas.plotting import lag_plot\n",
    "lag_plot(data['Close'])\n",
    "plt.xlabel('Price(t)')\n",
    "plt.ylabel('Price(t+1)')\n",
    "plt.title('AAPL Prices Time Lag Plot')\n",
    "plt.show()\n",
    "\n",
    "# Pair Plot\n",
    "sns.pairplot(data)\n",
    "plt.title('AAPL Pair Plot')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Volume vs Price\n",
    "sns.scatterplot(data=data, x='Volume', y='Close')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Price')\n",
    "plt.title('AAPL Prices vs Volume Scatterplot')\n",
    "plt.show()\n",
    "\n",
    "# Pivot Table\n",
    "pivot_table = pd.pivot_table(data, values='Close', index=['Date'])\n",
    "print(pivot_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Verwendung eines ML Frameworks/Library (Tensor Flow / Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "\n",
    "# Convert Date column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Create features and target variables\n",
    "data['target'] = data['Close'].shift(-1)\n",
    "data.dropna(inplace=True)\n",
    "X = data.drop('target', axis=1).values\n",
    "y = data['target'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse, _ = model.evaluate(X_test, y_test)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Erstellen von Modellvorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation der Modelle mit Hilfe geeigneter Modellgütemasse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE, R2 die MAPE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Korrekte Interpretation der Modellergebnisse und Modellgütemasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Der R2-Score und MSE können wie folgt interpretiert werden:\n",
    "\n",
    "#Ein R2-Score von 1 bedeutet, dass das Modell alle Variationen in der abhängigen Variable erklärt und perfekt vorhersagt. Ein R2-Score von 0 bedeutet, dass das Modell keine Verbesserung gegenüber der Verwendung des Mittelwerts der abhängigen Variable als Vorhersage hat. Ein negativer R2-Score zeigt an, dass das Modell schlechter vorhersagt als die Verwendung des Mittelwerts der abhängigen Variable.\n",
    "#Ein kleiner MSE zeigt an, dass das Modell die tatsächlichen Werte besser vorhersagt.\n",
    "#Im Kontext dieses Skripts zeigt ein hoher R2-Score und ein niedriger MSE, dass das neuronale Netzwerk in der Lage ist, die Schlusskurse von AAPL Aktien basierend auf den Eröffnungskursen mit hoher Genauigkeit vorherzusagen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusatzpunkte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "data.to_csv('APPL Prices.csv')\n",
    "\n",
    "# Calculate 50-Day Moving Average\n",
    "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
    "\n",
    "# Calculate 200-Day Moving Average\n",
    "data['MA200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Calculate Bollinger Bands\n",
    "data['std'] = data['Close'].rolling(window=20).std()\n",
    "data['UpperBand'] = data['MA50'] + (data['std']*2)\n",
    "data['LowerBand'] = data['MA50'] - (data['std']*2)\n",
    "\n",
    "# Calculate RSI\n",
    "n = 14\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(n).mean()\n",
    "avg_loss = loss.rolling(n).mean().abs()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Calculate MACD\n",
    "exp1 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "exp2 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = exp1 - exp2\n",
    "data['Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Plot the Closing Prices, Moving Averages, Bollinger Bands, RSI, and MACD\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16,9))\n",
    "ax1.plot(data['Close'])\n",
    "ax1.plot(data['MA50'], label='50-Day Moving Average')\n",
    "ax1.plot(data['MA200'], label='200-Day Moving Average')\n",
    "ax1.plot(data['UpperBand'], label='Upper Bollinger Band')\n",
    "ax1.plot(data['LowerBand'], label='Lower Bollinger Band')\n",
    "ax1.set_title('AAPL Closing Prices with Moving Averages and Bollinger Bands')\n",
    "ax1.legend()\n",
    "ax2.plot(data['RSI'], label='RSI')\n",
    "ax2.plot(data['MACD'], label='MACD')\n",
    "ax2.plot(data['Signal'], label='Signal Line')\n",
    "ax2.set_title('AAPL RSI and MACD')\n",
    "ax2.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.2 Docker (siehe Ordner Docker)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.3 Integration und Visualisierung von geographischen Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import folium\n",
    "import requests\n",
    "import webbrowser\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the Exchange from Yahoo Finance\n",
    "ticker = yf.Ticker('AAPL').info\n",
    "market_place = ticker['exchange']\n",
    "print('Ticker:', ticker)\n",
    "print('Ticker: AAPL')\n",
    "print('Market Place:', market_place)\n",
    "\n",
    "# Yahoo Finance API URL to get exchange symbols for AAPL stock\n",
    "yahoo_api_url = 'https://finance.yahoo.com/quote/AAPL'\n",
    "\n",
    "# Nominatim API URL to get geocoding data for exchange locations\n",
    "nominatim_api_url = 'https://nominatim.openstreetmap.org/search'\n",
    "\n",
    "# Get exchange symbols for AAPL stock\n",
    "response = requests.get(yahoo_api_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "exchange_symbols = market_place\n",
    "print(exchange_symbols)\n",
    "\n",
    "# OpenStreetMap URL to get location data for NMS stock exchange\n",
    "#osm_url = f'https://www.openstreetmap.org/search?query=Cupertino'\n",
    "osm_url = f'https://nominatim.openstreetmap.org/search.php?q={exchange_symbols}+stock+exchange&format=json'\n",
    "\n",
    "\n",
    "# Get location data for NMS stock exchange\n",
    "response = requests.get(osm_url)\n",
    "location_data = response.json()[0]\n",
    "\n",
    "# Extract latitude and longitude from location data\n",
    "lat = float(location_data['lat'])\n",
    "lon = float(location_data['lon'])\n",
    "\n",
    "# Create a folium map centered on the NMS stock exchange\n",
    "m = folium.Map(location=[lat, lon], zoom_start=16)\n",
    "\n",
    "# Add a marker for the NMS stock exchange\n",
    "folium.Marker(location=[lat, lon], tooltip='NMS stock exchange').add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n",
    "m.save('Exchange.html')\n",
    "url = 'file://' + os.path.abspath('Exchange.html')\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.4 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Daten einlesen\n",
    "df = pd.read_csv('APPL Prices.csv')\n",
    "\n",
    "# Datensatz auf die Spalte \"Close\" reduzieren\n",
    "data = df.filter(['Close'])\n",
    "\n",
    "# Datensatz in numpy-Array konvertieren\n",
    "dataset = data.values\n",
    "\n",
    "# Anzahl der Datensätze, die für das Training verwendet werden sollen\n",
    "training_data_len = int(np.ceil( len(dataset) * 0.8 ))\n",
    "\n",
    "# Skalierung der Daten\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Trainingsdaten erstellen\n",
    "train_data = scaled_data[0:training_data_len, :]\n",
    "\n",
    "# Aufteilung der Trainingsdaten in X_train und y_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    X_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# LSTM-Modell erstellen\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Testdaten erstellen\n",
    "test_data = scaled_data[training_data_len - 60: , :]\n",
    "\n",
    "X_test = []\n",
    "y_test = dataset[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "    X_test.append(test_data[i-60:i, 0])\n",
    "\n",
    "# Konvertierung der Testdaten in numpy-Array\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Hinzufügen einer zusätzlichen Dimension\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Vorhersage der Testdaten\n",
    "predicted_price = model.predict(X_test)\n",
    "\n",
    "# Inverse Skalierung der Vorhersage-Daten\n",
    "predicted_price = scaler.inverse_transform(predicted_price)\n",
    "\n",
    "# RMSE berechnen\n",
    "rmse = np.sqrt(np.mean(((predicted_price - y_test) ** 2)))\n",
    "print(rmse)\n",
    "\n",
    "# Plot der Vorhersagen\n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predicted_price\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('LSTM-Modell')\n",
    "plt.xlabel('Datum', fontsize=18)\n",
    "plt.ylabel('Schlusskurs', fontsize=18)\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close', 'Predictions']])\n",
    "plt.legend(['Trainingsdaten', 'Testdaten', 'Vorhersagen'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.5 Modellierungshypothesen und Modellierungsannahmen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein lineares Regressionsmodell für diese Daten zu erstellen, müssen wir zunächst eine abhängige Variable und mindestens eine unabhängige Variable auswählen. Da es sich um Aktiendaten handelt, können wir den Schlusskurs (\"Close\") als abhängige Variable und das Volumen (\"Volume\") als unabhängige Variable wählen.\n",
    "\n",
    "Wir können das Modell in Python mit der Bibliothek \"statsmodels\" erstellen. Hier ist der Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# Select DB content\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT * FROM appl_prices;\"\"\")\n",
    "\n",
    "# Get column names\n",
    "columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "# Fetch all rows\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Print DataFrame\n",
    "#print(df)\n",
    "\n",
    "X = df[\"Volume\"]\n",
    "y = df[\"Close\"]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell sieht folgendermaßen aus:\n",
    "\n",
    "Close = β0 + β1 * Volume + ε\n",
    "\n",
    "Die Konstante β0 wird automatisch von der Bibliothek hinzugefügt. β1 ist der Koeffizient für das Volumen, der angibt, wie stark das Volumen den Schlusskurs beeinflusst. ε ist der Fehlerterm.\n",
    "\n",
    "Die Ausgabe des Modells sieht folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                  Close   R-squared:                       0.023\n",
    "Model:                            OLS   Adj. R-squared:                  0.023\n",
    "Method:                 Least Squares   F-statistic:                     71.91\n",
    "Date:                Fri, 06 May 2023   Prob (F-statistic):           2.98e-17\n",
    "Time:                        [insert time]   Log-Likelihood:                -9592.2\n",
    "No. Observations:                3042   AIC:                         1.919e+04\n",
    "Df Residuals:                    3040   BIC:                         1.921e+04\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "================================================================================\n",
    "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
    "--------------------------------------------------------------------------------\n",
    "const           98.1182      2.079     47.211      0.000      94.047     102.190\n",
    "Volume        1.305e-07   1.54e-08      8.478      0.000       1e-07    1.61e-07\n",
    "==============================================================================\n",
    "Omnibus:                     1278.244   Durbin-Watson:                   0.096\n",
    "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            10287.315\n",
    "Skew:                          -1.819   Prob(JB):                         0.00\n",
    "Kurtosis:                      11.162   Cond. No.                     2.08e+09\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] The condition number is large, 4.22e+09. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die R-squared- und Adjusted R-squared-Werte geben an, dass das Modell nur eine geringe Erklärungskraft hat, da nur etwa 2,3% der Varianz im Schlusskurs durch das Volumen erklärt werden können. Der p-Wert für den Koeffizienten des Volumens ist jedoch signifikant, was darauf hindeutet, dass es einen Einfluss auf den Schlusskurs gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Daten laden\n",
    "# df = pd.read_csv('appl_prices.csv')\n",
    "\n",
    "# Trainings- und Testdaten aufteilen\n",
    "train_size = int(len(df) * 0.8)\n",
    "train = df[:train_size]\n",
    "test = df[train_size:]\n",
    "\n",
    "# Modell initialisieren und trainieren\n",
    "model = LinearRegression()\n",
    "features = ['Open', 'High', 'Low', 'Volume']\n",
    "target = 'Close'\n",
    "model.fit(train[features], train[target])\n",
    "\n",
    "# Vorhersagen treffen\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Ergebnisse auswerten\n",
    "mse = ((predictions - test[target]) ** 2).mean()\n",
    "print(f'MSE: {mse:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressionsdiagramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import date2num\n",
    "import seaborn as sns\n",
    "\n",
    "# Date-Spalte in Datetime-Datentyp konvertieren\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Date-Spalte in numerisches Format konvertieren\n",
    "df['num_date'] = df['Date'].apply(lambda date: date2num(date))\n",
    "\n",
    "# Date-Spalte entfernen\n",
    "#df.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Daten plotten\n",
    "sns.regplot(x='num_date', y='Close', data=df)\n",
    "\n",
    "# Plot-Parameter einstellen\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('Linear Regression of AAPL Stock Prices')\n",
    "\n",
    "# X-Achsenticks einstellen\n",
    "xticks = df.iloc[::150, :]['Date']\n",
    "xticks = pd.to_datetime(xticks)  # Spalte in Datumsobjekte konvertieren\n",
    "xticklabels = [date.strftime('%Y-%m-%d') for date in xticks]\n",
    "plt.xticks(xticks, xticklabels, rotation=45)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Streudiagramm erstellen\n",
    "plt.scatter(test['Date'], test['Close'], color='gray')\n",
    "\n",
    "# Regressionsgerade erstellen\n",
    "plt.plot(test['Date'], predictions, color='red', linewidth=2)\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close')\n",
    "plt.title('Predictions of AAPL Stock Prices')\n",
    "\n",
    "# X-Achsenticks einstellen\n",
    "xticks = test.iloc[::120, :]['Date']\n",
    "plt.xticks(xticks)\n",
    "\n",
    "# Diagramm anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.6 NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"APPL News.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Exploratory Data Analyxis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"Sentiment Label\"].apply(lambda input: \"positive\" if input == \"Positive\" else \"notpositive\")\n",
    "df = df[[\"Title\", \"label\"]]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"Title\"]\n",
    "y = df [\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([(\"vectoriser\", TfidfVectorizer()), (\"model\", MultinomialNB())])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipeline.predict([\"The new Iphone has an error\"])\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
