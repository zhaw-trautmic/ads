{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Datenerhebung mittels API & Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import datetime\n",
    "from pandas_datareader import data\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Yahoo Finance API: Aktienkurs [AAPL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date        Open        High         Low       Close   Adj Close  \\\n",
      "0     2010-01-04    7.622500    7.660714    7.585000    7.643214    6.496295   \n",
      "1     2010-01-05    7.664286    7.699643    7.616071    7.656429    6.507526   \n",
      "2     2010-01-06    7.656429    7.686786    7.526786    7.534643    6.404014   \n",
      "3     2010-01-07    7.562500    7.571429    7.466071    7.520714    6.392177   \n",
      "4     2010-01-08    7.510714    7.571429    7.466429    7.570714    6.434674   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "3037  2022-01-26  163.500000  164.389999  157.820007  159.690002  158.307526   \n",
      "3038  2022-01-27  162.449997  163.839996  158.279999  159.220001  157.841599   \n",
      "3039  2022-01-28  165.710007  170.350006  162.800003  170.330002  168.855423   \n",
      "3040  2022-01-31  170.160004  175.000000  169.509995  174.779999  173.266876   \n",
      "3041  2022-02-01  174.009995  174.839996  172.309998  174.610001  173.098358   \n",
      "\n",
      "         Volume  \n",
      "0     493729600  \n",
      "1     601904800  \n",
      "2     552160000  \n",
      "3     477131200  \n",
      "4     447610800  \n",
      "...         ...  \n",
      "3037  108275300  \n",
      "3038  121954600  \n",
      "3039  179935700  \n",
      "3040  115541600  \n",
      "3041   86213900  \n",
      "\n",
      "[3042 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance according Parameters (ticket, period, interval)\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "\n",
    "# Save data to a CSV file\n",
    "data.to_csv('AAPL Prices.csv')\n",
    "\n",
    "# Visualize Data\n",
    "print(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Web Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Get from Finanznachrichten the Potenzial of Apple Inc. shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UBS  163,21 +1 %\n",
      "JEFFERIES  176,66 +9 %\n",
      "CREDIT SUISSE  170,32 +5 %\n",
      "GOLDMAN SACHS  189,24 +17 %\n",
      "JPMORGAN  172,04 +6 %\n",
      "DEUTSCHE BANK RESEARCH  146,35 -10 %\n",
      "BERNSTEIN RESEARCH  114,97 -29 %\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.finanznachrichten.de/aktien-analysen/apple-inc.htm\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table = soup.find(\"div\", attrs={\"class\": \"seitenelement Analysen_Analysten poslinks\", \"id\": \"W620\"}).find(\"table\")\n",
    "\n",
    "for row in table.find_all(\"tr\")[1:]:\n",
    "    analyst = row.find_all(\"td\")[0].text.strip()\n",
    "    rating = row.find_all(\"td\")[1].text.strip()\n",
    "    price_target = row.find_all(\"td\")[2].text.strip()\n",
    "    date = row.find_all(\"td\")[3].text.strip()\n",
    "    print(analyst, rating, price_target, date)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Fetch from Politifact Apple Inc. News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "https://www.politifact.com/factchecks/list/?page=1\n",
      "30\n",
      "processing page : 2\n",
      "https://www.politifact.com/factchecks/list/?page=2\n",
      "30\n",
      "processing page : 3\n",
      "https://www.politifact.com/factchecks/list/?page=3\n",
      "30\n",
      "processing page : 4\n",
      "https://www.politifact.com/factchecks/list/?page=4\n",
      "30\n",
      "processing page : 5\n",
      "https://www.politifact.com/factchecks/list/?page=5\n",
      "30\n",
      "processing page : 6\n",
      "https://www.politifact.com/factchecks/list/?page=6\n",
      "30\n",
      "processing page : 7\n",
      "https://www.politifact.com/factchecks/list/?page=7\n",
      "30\n",
      "processing page : 8\n",
      "https://www.politifact.com/factchecks/list/?page=8\n",
      "30\n",
      "processing page : 9\n",
      "https://www.politifact.com/factchecks/list/?page=9\n",
      "30\n",
      "processing page : 10\n",
      "https://www.politifact.com/factchecks/list/?page=10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "\n",
    "pagesToGet = 10 # Check amout of pages\n",
    "search_term = \"Latino\" # Check for key word\n",
    "upperframe = [] # This line initializes an empty list called\n",
    "\n",
    "# Stores the data in a CSV format\n",
    "filename = \"APPL_NEWS_raw.csv\"\n",
    "f = open(filename, \"w\", encoding='utf-8') # Write mode and encoding\n",
    "headers = \"Statement,Link,Date,Source,Label\\n\"\n",
    "f.write(headers)\n",
    "\n",
    "# Itirates through a loop of the defined no. of pages\n",
    "for page in range(1, pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.politifact.com/factchecks/list/?page=' + str(page) # For each page number, it constructs a URL specific to that page by appending the page number to the base URL of the Politifact fact-checking list.\n",
    "    print(url)\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "    except Exception as e:              # Show connection errors\n",
    "        error_type, error_obj, error_info = sys.exc_info()\n",
    "        print ('ERROR FOR LINK:', url)\n",
    "        print (error_type, 'Line:', error_info.tb_lineno)\n",
    "        continue\n",
    "    time.sleep(2)                       # Pause\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    links = soup.find_all('li', attrs={'class': 'o-listicle__item'})  # It searches for all <li> elements with the attribute class=\"o-listicle__item\", which likely represent ''Apple' fact-checking items on the page.\n",
    "    print(len(links))\n",
    "\n",
    "# This code extracts specific information from elements in a web page, stores the extracted values in variables and a list, and writes the information to the CSV file in a comma-separated format.\n",
    "    for j in links: # It iterates over a collection called links, which is defined above\n",
    "        Statement = j.find(\"div\", attrs={'class': 'm-statement__quote'}).text.strip() # It extracts the text content from a <div> element with the class \"m-statement__xxx\" and removes any leading or trailing whitespace\n",
    "        if search_term.lower() not in Statement.lower(): # It checks if the search_term 'APPL' is present in the Statement text. If the search term is not found (both the search term and Statement are converted to lowercase for case-insensitive comparison), the loop continues to the next iteration.\n",
    "            continue\n",
    "        Link = \"https://www.politifact.com\"\n",
    "        Link += j.find(\"div\", attrs={'class': 'm-statement__quote'}).find('a')['href'].strip()\n",
    "        Date = j.find('div', attrs={'class': 'm-statement__body'}).find('footer').text[-14:-1].strip()\n",
    "        Source = j.find('div', attrs={'class': 'm-statement__meta'}).find('a').text.strip()\n",
    "        Label = j.find('div', attrs={'class': 'm-statement__content'}).find('img', attrs={'class': 'c-image__original'}).get('alt').strip()\n",
    "        upperframe.append((Statement, Link, Date, Source, Label)) # It appends a tuple containing the four values\n",
    "        f.write(Statement.replace(\",\", \"^\") + \",\" + Link + \",\" + Date.replace(\",\", \"^\") + \",\" + Source.replace(\",\", \"^\") + \",\" + Label.replace(\",\", \"^\") + \"\\n\")\n",
    "f.close()   \n",
    "\n",
    "# Erstellen eines DataFrames aus den Ergebnissen\n",
    "data = pd.DataFrame(upperframe, columns=['Statement', 'Link', 'Date', 'Source', 'Label'])\n",
    "\n",
    "# Öffnen einer neuen Datei, um die Sentiments der Überschriften zu speichern\n",
    "with open('APPL_News.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Sentiment', 'Sentiment Label'])  # Schreibt die Spaltenüberschriften\n",
    "    for title in data['Statement']:\n",
    "        blob = TextBlob(title)\n",
    "        sentiment = blob.sentiment.polarity\n",
    "        if sentiment < -0.2:\n",
    "            sentiment_label = 'negativ'\n",
    "        elif sentiment > 0.2:\n",
    "            sentiment_label = 'positiv'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "        writer.writerow([title, sentiment, sentiment_label])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Datenaufbereitung"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entfernen NAs und Duplikate, Erstellen neuer Variablen, Anreicherung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types:\n",
      "Date         datetime64[ns]\n",
      "Open                float64\n",
      "High                float64\n",
      "Low                 float64\n",
      "Close               float64\n",
      "Adj Close           float64\n",
      "Volume                int64\n",
      "dtype: object\n",
      "\n",
      "Data Dimensions:\n",
      "(3042, 7)\n",
      "---------------------------BEFORE CLEAN---------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3042 entries, 0 to 3041\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Date       3042 non-null   datetime64[ns]\n",
      " 1   Open       3042 non-null   float64       \n",
      " 2   High       3042 non-null   float64       \n",
      " 3   Low        3042 non-null   float64       \n",
      " 4   Close      3042 non-null   float64       \n",
      " 5   Adj Close  3042 non-null   float64       \n",
      " 6   Volume     3042 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
      "memory usage: 166.5 KB\n",
      "---------------------------AFTER CLEAN---------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3042 entries, 0 to 3041\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Date       3042 non-null   datetime64[ns]\n",
      " 1   Open       3042 non-null   float64       \n",
      " 2   High       3042 non-null   float64       \n",
      " 3   Low        3042 non-null   float64       \n",
      " 4   Close      3042 non-null   float64       \n",
      " 5   Adj Close  3042 non-null   float64       \n",
      " 6   Volume     3042 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(5), int64(1)\n",
      "memory usage: 190.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance according Parameters (ticket, period, interval)\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "\n",
    "# Save data to a CSV file\n",
    "data.to_csv('AAPL Prices.csv')\n",
    "\n",
    "# Cleaning der Quotes\n",
    "df = data\n",
    "\n",
    "# Using the pandas library in Python to convert the 'Date' column of a DataFrame (ref. as df) into a datetime data type (enriching the data with correct date format).\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "raw = df\n",
    "\n",
    "# Show data types and dimensions\n",
    "print(\"Data Types:\")\n",
    "print(data.dtypes)\n",
    "print(\"\\nData Dimensions:\")\n",
    "print(data.shape)\n",
    "\n",
    "# Cleaning Process\n",
    "print('---------------------------BEFORE CLEAN---------------------------')\n",
    "raw.info()\n",
    "print('---------------------------AFTER CLEAN---------------------------')\n",
    "\n",
    "# Data cleaning ('drop_duplicates' / 'dropna') and missing values using forward ('fill')\n",
    "df = df.drop_duplicates()\n",
    "df = df\n",
    "df = df.dropna()\n",
    "df = df\n",
    "df = df.ffill()\n",
    "df = df\n",
    "clean = df\n",
    "clean.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 DB - PostgreSQL DB initiate -> In Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "could not translate host name \"db\" to address: Unknown host\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# Connect DB\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m conn \u001b[39m=\u001b[39m psycopg2\u001b[39m.\u001b[39;49mconnect(\u001b[39m\"\u001b[39;49m\u001b[39mhost=db dbname=postgres user=admin password=secret\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Von Aussen\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m#conn = psycopg2.connect(\"host=http://localhost:5432/ dbname=postgres user=admin password=secret\")\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[39m# Insert data to appl_prices\u001b[39;00m\n\u001b[0;32m     22\u001b[0m engine \u001b[39m=\u001b[39m create_engine(\u001b[39m'\u001b[39m\u001b[39mpostgresql://admin:secret@db:5432/postgres\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Trautmann\\anaconda3\\envs\\ads_env\\lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39mconnection_factory, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: could not translate host name \"db\" to address: Unknown host\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import fnmatch\n",
    "import tempfile\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "os.environ['MPLCONFIGDIR'] = \"/home/jovyan\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Connect DB\n",
    "conn = psycopg2.connect(\"host=db dbname=postgres user=admin password=secret\")\n",
    "# Von Aussen\n",
    "#conn = psycopg2.connect(\"host=http://localhost:5432/ dbname=postgres user=admin password=secret\")\n",
    "\n",
    "# Insert data to appl_prices\n",
    "engine = create_engine('postgresql://admin:secret@db:5432/postgres')\n",
    "data.to_sql('appl_prices', engine, if_exists='replace')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Spalte positive hinzufügen\n",
    "cur.execute(\"ALTER TABLE appl_prices ADD COLUMN Positive INTEGER DEFAULT 0;\")\n",
    "\n",
    "# Änderungen speichern\n",
    "conn.commit()\n",
    "\n",
    "# Datenbankverbindung schliessen\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect DB\n",
    "conn = psycopg2.connect(\"host=db dbname=postgres user=admin password=secret\")\n",
    "\n",
    "# Update DB\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"UPDATE appl_prices SET positive = CASE WHEN \"Close\" >= \"Open\" THEN 1 ELSE 0 END;\"\"\")\n",
    "\n",
    "# Änderungen speichern\n",
    "conn.commit()\n",
    "\n",
    "# Datenbankverbindung schliessen\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect DB\n",
    "conn = psycopg2.connect(\"host=db dbname=postgres user=admin password=secret\")\n",
    "\n",
    "# Selct DB content\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"SELECT * FROM appl_prices LIMIT 10;\"\"\")\n",
    "\n",
    "rows = cur.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Datenbankverbindung schliessen\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bsp. Histogramm und weiter Grafiken, Zeitreihen, lags vom close preis, Volumen, volumen mit preis vergleichen \n",
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "data.to_csv('AAPL Prices.csv')\n",
    "print(data)\n",
    "\n",
    "# Histogram (number of trades executed vs. price per APPL share)\n",
    "plt.hist(data['Close'], bins=50)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('AAPL Prices Histogram')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot (based on the closing price)\n",
    "plt.boxplot(data['Close'], vert=False) # creates a horizontal box plot \n",
    "plt.xlabel('Price')\n",
    "plt.title('AAPL Prices Boxplot')\n",
    "plt.show()\n",
    "\n",
    "# Time Series (different types of plots using different libraries 'matplotlib')\n",
    "plt.plot(data['Date'], data['Close'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('AAPL Prices Time Series')\n",
    "plt.show()\n",
    "\n",
    "# Lineplot (different types of plots using different libraries 'seaborn')\n",
    "sns.lineplot(data=data, x='Date', y='Close')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('AAPL Prices Lineplot')\n",
    "plt.show()\n",
    "\n",
    "# Time Lag Plot (this code creates a lag plot to visualize the relationship between the lagged 'Close' prices at time t and the 'Close' prices at time t+1 (next day) for the AAPL stock)\n",
    "from pandas.plotting import lag_plot\n",
    "lag_plot(data['Close'])\n",
    "plt.xlabel('Price(t)')\n",
    "plt.ylabel('Price(t+1)')\n",
    "plt.title('AAPL Prices Time Lag Plot')\n",
    "plt.show()\n",
    "\n",
    "# Pair Plot (Plot pairwise relationships between multiple variables in a dataset - scatter plot matrix)\n",
    "sns.pairplot(data)\n",
    "plt.title('AAPL Pair Plot')\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Price vs Volume\n",
    "sns.scatterplot(data=data, x='Volume', y='Close')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Price')\n",
    "plt.title('AAPL Prices vs Volume Scatterplot')\n",
    "plt.show()\n",
    "\n",
    "# Pivot Table (creates a pivot table from the data dataset - date with the closing price) \n",
    "pivot_table = pd.pivot_table(data, values='Close', index=['Date'])\n",
    "print(pivot_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Verwendung eines ML Frameworks/Library (Tensor Flow / Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "\n",
    "# Convert Date column to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Create features and target variables\n",
    "data['target'] = data['Close'].shift(-1)\n",
    "data.dropna(inplace=True)\n",
    "X = data.drop('target', axis=1).values\n",
    "y = data['target'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(model.predict(X_test))\n",
    "rmse = np.sqrt(np.mean(np.square(y_test_unscaled - y_pred_unscaled)))\n",
    "mape = np.mean(np.abs((y_test_unscaled - y_pred_unscaled) / y_test_unscaled)) * 100\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "print('Mean Absolute Percentage Error:', mape)\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Erstellen von Modellvorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "\n",
    "# Data preparation\n",
    "close_prices = data['Close'].values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "close_prices_scaled = scaler.fit_transform(close_prices)\n",
    "\n",
    "lookback = 60  # lookback window for the LSTM model\n",
    "X, y = [], []\n",
    "for i in range(len(close_prices_scaled) - lookback - 1):\n",
    "    X.append(close_prices_scaled[i:(i + lookback), 0])\n",
    "    y.append(close_prices_scaled[i + lookback, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Define early stopping and model checkpoint callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stop, model_checkpoint])\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test, y_test)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute R^2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('R^2 Score:', r2)\n",
    "\n",
    "# Compute MAPE score\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print('MAPE:', mape)\n",
    "\n",
    "# Compute Root Squared Error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Root Squared Error:', rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation der Modelle mit Hilfe geeigneter Modellgütemasse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "\n",
    "# Define input and output variables\n",
    "X = data[['Open', 'High', 'Low', 'Volume']].values\n",
    "y = data['Close'].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and compile the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Fit the model on the training set\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the model performance metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the model performance metrics\n",
    "print(f'R2 Score: {r2:.4f}')\n",
    "print(f'MAPE: {mape:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Korrekte Interpretation der Modellergebnisse und Modellgütemasse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Der R2-Score und MSE können wie folgt interpretiert werden:\n",
    "\n",
    "#Ein R2-Score von 1 bedeutet, dass das Modell alle Variationen in der abhängigen Variable erklärt und perfekt vorhersagt. Ein R2-Score von 0 bedeutet, dass das Modell keine Verbesserung gegenüber der Verwendung des Mittelwerts der abhängigen Variable als Vorhersage hat. Ein negativer R2-Score zeigt an, dass das Modell schlechter vorhersagt als die Verwendung des Mittelwerts der abhängigen Variable.\n",
    "#Ein kleiner MSE zeigt an, dass das Modell die tatsächlichen Werte besser vorhersagt.\n",
    "#Im Kontext dieses Skripts zeigt ein hoher R2-Score und ein niedriger MSE, dass das neuronale Netzwerk in der Lage ist, die Schlusskurse von AAPL Aktien basierend auf den Eröffnungskursen mit hoher Genauigkeit vorherzusagen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusatzpunkte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import date2num\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Get Ticket Quotes from Yahoo Finance\n",
    "ticker = 'AAPL'\n",
    "period1 = int(time.mktime(datetime.datetime(2010, 1, 1, 23, 59).timetuple()))\n",
    "period2 = int(time.mktime(datetime.datetime(2022, 2, 1, 23, 59).timetuple()))\n",
    "interval = '1d'\n",
    "query_string = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={period1}&period2={period2}&interval={interval}&events=history&includeAdjustedClose=true'\n",
    "data = pd.read_csv(query_string)\n",
    "data.to_csv('AAPL Prices.csv')\n",
    "\n",
    "# Calculate 50-Day Moving Average (indicator that calculates the average price of a security (such as a stock) over a 50-day period.)\n",
    "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
    "\n",
    "# Calculate 200-Day Moving Average (indicator that calculates the average price of a security (such as a stock) over a 200-day period.)\n",
    "data['MA200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Calculate Bollinger Bands (Upper band and a lower band to analyze price volatility and potential price reversal points)\n",
    "data['std'] = data['Close'].rolling(window=20).std()\n",
    "data['UpperBand'] = data['MA50'] + (data['std']*2)\n",
    "data['LowerBand'] = data['MA50'] - (data['std']*2)\n",
    "\n",
    "# Calculate RSI (measure the strength and speed of price movements of the AAPL stock)\n",
    "n = 14\n",
    "delta = data['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(n).mean()\n",
    "avg_loss = loss.rolling(n).mean().abs()\n",
    "rs = avg_gain / avg_loss\n",
    "data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Calculate MACD (identify potential buying and selling opportunities of the AAPL in the market)\n",
    "# Calculate the Singal Line: Is used to generate trading signals or identify potential buying or selling opportunities\n",
    "exp1 = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "exp2 = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "data['MACD'] = exp1 - exp2\n",
    "data['Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Plot the Closing Prices, Moving Averages, Bollinger Bands, RSI, and MACD\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 9))\n",
    "\n",
    "# Format the x-axis as dates\n",
    "date_format = mdates.DateFormatter('%Y-%m-%d')\n",
    "ax1.xaxis.set_major_formatter(date_format)\n",
    "ax2.xaxis.set_major_formatter(date_format)\n",
    "\n",
    "ax1.plot(data['Date'], data['Close'])\n",
    "ax1.plot(data['Date'], data['MA50'], label='50-Day Moving Average')\n",
    "ax1.plot(data['Date'], data['MA200'], label='200-Day Moving Average')\n",
    "ax1.plot(data['Date'], data['UpperBand'], label='Upper Bollinger Band')\n",
    "ax1.plot(data['Date'], data['LowerBand'], label='Lower Bollinger Band')\n",
    "ax1.set_title('AAPL Closing Prices with Moving Averages and Bollinger Bands')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(data['Date'], data['RSI'], label='RSI')\n",
    "ax2.plot(data['Date'], data['MACD'], label='MACD')\n",
    "ax2.plot(data['Date'], data['Signal'], label='Signal Line')\n",
    "ax2.set_title('AAPL RSI and MACD')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.2 Docker (siehe Ordner Docker)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.3 Integration und Visualisierung von geographischen Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import folium\n",
    "import requests\n",
    "import webbrowser\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the Exchange from Yahoo Finance\n",
    "ticker = yf.Ticker('AAPL').info\n",
    "market_place = ticker['exchange']\n",
    "print('Ticker:', ticker)\n",
    "print('Ticker: AAPL')\n",
    "print('Market Place:', market_place)\n",
    "\n",
    "# Yahoo Finance API URL to get exchange symbols for AAPL stock\n",
    "yahoo_api_url = 'https://finance.yahoo.com/quote/AAPL'\n",
    "\n",
    "# Nominatim API URL to get geocoding data for exchange locations\n",
    "nominatim_api_url = 'https://nominatim.openstreetmap.org/search'\n",
    "\n",
    "# Get exchange symbols for AAPL stock\n",
    "response = requests.get(yahoo_api_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "exchange_symbols = market_place\n",
    "print(exchange_symbols)\n",
    "\n",
    "# OpenStreetMap URL to get location data for NMS stock exchange\n",
    "#osm_url = f'https://www.openstreetmap.org/search?query=Cupertino'\n",
    "osm_url = f'https://nominatim.openstreetmap.org/search.php?q={exchange_symbols}+stock+exchange&format=json'\n",
    "\n",
    "# Get location data for NMS stock exchange\n",
    "response = requests.get(osm_url)\n",
    "location_data = response.json()[0]\n",
    "\n",
    "# Extract latitude and longitude from location data\n",
    "lat = float(location_data['lat'])\n",
    "lon = float(location_data['lon'])\n",
    "\n",
    "# Create a folium map centered on the NMS stock exchange\n",
    "m = folium.Map(location=[lat, lon], zoom_start=16)\n",
    "\n",
    "# Add a marker for the NMS stock exchange\n",
    "folium.Marker(location=[lat, lon], tooltip='NMS stock exchange').add_to(m)\n",
    "\n",
    "# Display the map\n",
    "m\n",
    "m.save('Exchange.html')\n",
    "url = 'file://' + os.path.abspath('Exchange.html')\n",
    "webbrowser.open(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.4 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotheken importieren\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# Daten einlesen\n",
    "df = pd.read_csv('AAPL Prices.csv')\n",
    "\n",
    "# Datensatz auf die Spalte \"Close\" reduzieren\n",
    "data = df.filter(['Close'])\n",
    "\n",
    "# Datensatz in numpy-Array konvertieren\n",
    "dataset = data.values\n",
    "\n",
    "# Anzahl der Datensätze, die für das Training verwendet werden sollen\n",
    "training_data_len = int(np.ceil( len(dataset) * 0.8 ))\n",
    "\n",
    "# Skalierung der Daten\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Trainingsdaten erstellen\n",
    "train_data = scaled_data[0:training_data_len, :]\n",
    "\n",
    "# Aufteilung der Trainingsdaten in X_train und y_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    X_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# LSTM-Modell erstellen\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Modell kompilieren\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Testdaten erstellen\n",
    "test_data = scaled_data[training_data_len - 60: , :]\n",
    "\n",
    "X_test = []\n",
    "y_test = dataset[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "    X_test.append(test_data[i-60:i, 0])\n",
    "\n",
    "# Konvertierung der Testdaten in numpy-Array\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Hinzufügen einer zusätzlichen Dimension\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Vorhersage der Testdaten\n",
    "predicted_price = model.predict(X_test)\n",
    "\n",
    "# Inverse Skalierung der Vorhersage-Daten\n",
    "predicted_price = scaler.inverse_transform(predicted_price)\n",
    "\n",
    "# RMSE berechnen\n",
    "rmse = np.sqrt(np.mean(((predicted_price - y_test) ** 2)))\n",
    "print(rmse)\n",
    "\n",
    "# Plot der Vorhersagen\n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predicted_price\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('LSTM-Modell')\n",
    "plt.xlabel('Datum', fontsize=18)\n",
    "plt.ylabel('Schlusskurs', fontsize=18)\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close', 'Predictions']])\n",
    "plt.legend(['Trainingsdaten', 'Testdaten', 'Vorhersagen'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.5 Modellierungshypothesen und Modellierungsannahmen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein lineares Regressionsmodell für diese Daten zu erstellen, müssen wir zunächst eine abhängige Variable und mindestens eine unabhängige Variable auswählen. Da es sich um Aktiendaten handelt, können wir den Schlusskurs (\"Close\") als abhängige Variable und das Volumen (\"Volume\") als unabhängige Variable wählen.\n",
    "\n",
    "Wir können das Modell in Python mit der Bibliothek \"statsmodels\" erstellen. Hier ist der Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Daten einlesen\n",
    "df = pd.read_csv('AAPL Prices.csv')\n",
    "\n",
    "X = df[\"Volume\"]\n",
    "y = df[\"Close\"]\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Modell sieht folgendermaßen aus:\n",
    "\n",
    "Close = β0 + β1 * Volume + ε\n",
    "\n",
    "Die Konstante β0 wird automatisch von der Bibliothek hinzugefügt. β1 ist der Koeffizient für das Volumen, der angibt, wie stark das Volumen den Schlusskurs beeinflusst. ε ist der Fehlerterm.\n",
    "\n",
    "Die Ausgabe des Modells sieht folgendermaßen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:                  Close   R-squared:                       0.023\n",
    "Model:                            OLS   Adj. R-squared:                  0.023\n",
    "Method:                 Least Squares   F-statistic:                     71.91\n",
    "Date:                Fri, 06 May 2023   Prob (F-statistic):           2.98e-17\n",
    "Time:                        [insert time]   Log-Likelihood:                -9592.2\n",
    "No. Observations:                3042   AIC:                         1.919e+04\n",
    "Df Residuals:                    3040   BIC:                         1.921e+04\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "================================================================================\n",
    "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
    "--------------------------------------------------------------------------------\n",
    "const           98.1182      2.079     47.211      0.000      94.047     102.190\n",
    "Volume        1.305e-07   1.54e-08      8.478      0.000       1e-07    1.61e-07\n",
    "==============================================================================\n",
    "Omnibus:                     1278.244   Durbin-Watson:                   0.096\n",
    "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            10287.315\n",
    "Skew:                          -1.819   Prob(JB):                         0.00\n",
    "Kurtosis:                      11.162   Cond. No.                     2.08e+09\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] The condition number is large, 4.22e+09. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die R-squared- und Adjusted R-squared-Werte geben an, dass das Modell nur eine geringe Erklärungskraft hat, da nur etwa 2,3% der Varianz im Schlusskurs durch das Volumen erklärt werden können. Der p-Wert für den Koeffizienten des Volumens ist jedoch signifikant, was darauf hindeutet, dass es einen Einfluss auf den Schlusskurs gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Daten laden\n",
    "# df = pd.read_csv('appl_prices.csv')\n",
    "\n",
    "# Trainings- und Testdaten aufteilen\n",
    "train_size = int(len(df) * 0.8)\n",
    "train = df[:train_size]\n",
    "test = df[train_size:]\n",
    "\n",
    "# Modell initialisieren und trainieren\n",
    "model = LinearRegression()\n",
    "features = ['Open', 'High', 'Low', 'Volume']\n",
    "target = 'Close'\n",
    "model.fit(train[features], train[target])\n",
    "\n",
    "# Vorhersagen treffen\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Ergebnisse auswerten\n",
    "mse = ((predictions - test[target]) ** 2).mean()\n",
    "print(f'MSE: {mse:.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressionsdiagramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import date2num\n",
    "import seaborn as sns\n",
    "\n",
    "# Date-Spalte in Datetime-Datentyp konvertieren\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Date-Spalte in numerisches Format konvertieren\n",
    "df['num_date'] = df['Date'].apply(lambda date: date2num(date))\n",
    "\n",
    "# Date-Spalte entfernen\n",
    "#df.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Daten plotten\n",
    "sns.regplot(x='num_date', y='Close', data=df)\n",
    "\n",
    "# Plot-Parameter einstellen\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('Linear Regression of AAPL Stock Prices')\n",
    "\n",
    "# X-Achsenticks einstellen\n",
    "xticks = df.iloc[::150, :]['Date']\n",
    "xticks = pd.to_datetime(xticks)  # Spalte in Datumsobjekte konvertieren\n",
    "xticklabels = [date.strftime('%Y-%m-%d') for date in xticks]\n",
    "plt.xticks(xticks, xticklabels, rotation=45)\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorhersage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# test = df\n",
    "# # Streudiagramm erstellen\n",
    "# plt.scatter(test['Date'], test['Close'], color='gray')\n",
    "\n",
    "# # Regressionsgerade erstellen\n",
    "# plt.plot(test['Date'].iloc[:len(predictions)], predictions, color='red', linewidth=2)\n",
    "# #plt.plot(test['Date'], predictions, color='red', linewidth=2)\n",
    "\n",
    "# # Achsenbeschriftungen\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Closing Price')\n",
    "# plt.title('Predictions of AAPL Stock Prices')\n",
    "\n",
    "# # X-Achsenticks einstellen\n",
    "# xticks = test.iloc[::120, :]['Date']\n",
    "# xticklabels = [date.strftime('%Y-%m-%d') for date in xticks]\n",
    "# plt.xticks(xticks, xticklabels, rotation=45)\n",
    "\n",
    "# # Diagramm anzeigen\n",
    "# plt.show()\n",
    "\n",
    "# Diagramm erstellen\n",
    "plt.plot(test['Date'], predictions, color='red', linewidth=2)\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.title('Predictions of AAPL Stock Prices')\n",
    "\n",
    "# X-Achsenticks einstellen\n",
    "xticks = test.iloc[::120]['Date']\n",
    "xticklabels = [date for date in xticks]\n",
    "plt.xticks(xticks, xticklabels, rotation=45)\n",
    "\n",
    "# Diagramm anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z.6 NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"apple-twitter-texts.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Exploratory Data Analyxis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"sentiment\"].apply(lambda input: \"positive\" if input == 1 else \"notpositive\")\n",
    "df = df[[\"text\", \"label\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df [\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([(\"vectoriser\", TfidfVectorizer()), (\"model\", MultinomialNB())])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_predication = pipeline.predict(X_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_pred=y_predication, y_true=y_test)\n",
    "confusion = metrics.confusion_matrix(y_pred=y_predication, y_true=y_test)\n",
    "\n",
    "print(accuracy)\n",
    "print(confusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pipeline.predict([\"Apple is a bad company\"])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
